### General
# in debug_mode no agent directories will be created and no models will be saved
# further no wandb logging and fake (simulated) multiprocessing for traceback
debug_mode: false
# number of parallel environments
n_envs: 16
# gpu yes or no
no_gpu: false

### Training Monitoring
monitoring:
  # weights and biases logging
  use_wandb: true
  # save evaluation stats during training in log file
  eval_log: false

### General Training
# navigation task mode, chose from "random" or "staged"
task_mode: "dynamic_map_staged"
# number of simulation timesteps
n_timesteps: 20000000
max_num_moves_per_eps: 150

goal_radius: 0.5


callbacks:
  ### Periodic Eval
  periodic_eval:
    # max number of steps per episode
    max_num_moves_per_eps: 250
    # number of evaluation episodes
    n_eval_episodes: 70
    # evaluation frequency, evaluation after every n_envs * eval_freq timesteps
    eval_freq: 1000

  ### Training Curriculum
  # threshold metric to be considered during evaluation
  # can be either "succ" (success rate) or "rew" (reward)
  training_curriculum:
    # file for the robot's learning curriculum
    training_curriculum_file: "barn.yaml"
    curr_stage: 10
    threshold_type: "succ"
    upper_threshold: 0.9
    lower_threshold: 0.5

  ### Stop Training on Threshold
  # stops training when last stage reached and threshold satisfied
  stop_training:
    threshold_type: "succ"
    threshold: 0.95

### Agent Specs: Training Hyperparameter and Network Architecture
rl_agent:
  # name of architecture defined in the Policy factory
  architecture_name: "AGENT_54"
  # path to latest checkpoint; if provided the training will be resumed from that checkpoint
  resume: null # "jackal_AGENT_25_RobotSpecificEncoder_2023_01_21__03_01"

  frame_stacking:
    enabled: true
    stack_size: 16

  normalize: false
  laser:
    full_range_laser: true  # additional laser covering 360Â° covering blind spots -> additional collision check
    reduce_num_beams:
      enabled: true
      num_beams: 160

  reward_fnc: "rule_11"

  action_space:
    discrete: true
    custom_discretization: # only used if discrete is true, otherwise ignored
      enabled: true
      # number of buckets for each action dimension
      # (only non-holonomic robots with 2D action space supported)
      # e.g. given linear range [-1, 1], angular [-0.5, 0.5]
      # with buckets_linear_vel = 4 and buckets_angular_vel = 3
      # np.linspace of linear range: [-1, -0.33, 0.33, 1]
      # np.linspace of angular range: [-0.5, 0, 0.5]
      # resulting in 4*3 = 12 discrete actions + 1 for no action
      buckets_linear_vel: 10
      buckets_angular_vel: 13

  ppo:
    batch_size: 15840
    gamma: 0.99
    n_steps: 1200
    ent_coef: 0.005
    learning_rate: 0.0003
    vf_coef: 0.22
    max_grad_norm: 0.5
    gae_lambda: 0.95
    m_batch_size: 60
    n_epochs: 3
    clip_range: 0.22
